{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.virtualenvs/training/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/.virtualenvs/training/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/.virtualenvs/training/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/.virtualenvs/training/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/.virtualenvs/training/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/.virtualenvs/training/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Image data of dtype object cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e8aca7e79c6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgenerated_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/training/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2649\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[0;32m-> 2651\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2652\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/training/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/training/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/training/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/training/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5613\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5615\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5616\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5617\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/training/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    692\u001b[0m                 not np.can_cast(self._A.dtype, float, \"same_kind\")):\n\u001b[1;32m    693\u001b[0m             raise TypeError(\"Image data of dtype {} cannot be converted to \"\n\u001b[0;32m--> 694\u001b[0;31m                             \"float\".format(self._A.dtype))\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         if not (self._A.ndim == 2\n",
      "\u001b[0;31mTypeError\u001b[0m: Image data of dtype object cannot be converted to float"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.00140434]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# We will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    "\n",
    "    # Produce images for the GIF as we go\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch\n",
    "  display.clear_output(wait=True)\n",
    "  generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 2 is 240.46169066429138 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-d152560ca122>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-802af7bf198a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m       \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Produce images for the GIF as we go\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tensorflow2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tensorflow2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tensorflow2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tensorflow2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tensorflow2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tensorflow2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.virtualenvs/tensorflow2/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(epoch_no):\n",
    "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = 'dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "  filenames = glob.glob('image*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  last = -1\n",
    "  for i,filename in enumerate(filenames):\n",
    "    frame = 2*(i**0.5)\n",
    "    if round(frame) > round(last):\n",
    "      last = frame\n",
    "    else:\n",
    "      continue\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)\n",
    "\n",
    "import IPython\n",
    "if IPython.version_info > (6,2,0,''):\n",
    "  display.Image(filename=anim_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
